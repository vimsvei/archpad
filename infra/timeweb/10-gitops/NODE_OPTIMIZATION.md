# Оптимизация размещения подов на нодах

## Проблема

На кластере запущено ~50 подов, которые распределены по всем 6 нодам. Это неэффективно, так как:
- Ресурсы распределены неравномерно
- Сложнее управлять и мониторить
- Неиспользуемые ноды тратят ресурсы

## Решение

Использовать `podAffinity` для группировки подов приложений на меньшем количестве нод (3-4 ноды).

### Стратегия

1. **Группировка основных приложений** на 3-4 нодах через `podAffinity`
2. **Системные поды** (kube-system, argocd, traefik) могут оставаться на всех нодах
3. **DaemonSets** (если есть) должны быть на всех нодах

### Реализация

Добавить `podAffinity` в spec всех основных deployment:

```yaml
spec:
  template:
    spec:
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - arch-repo-service
                        - tenant-service
                        - portal
                        - hasura
                        - tolgee
                        - mailpit
                        - kratos
                        - hydra
                        - oathkeeper
                topologyKey: kubernetes.io/hostname
```

### Преимущества

- **Группировка**: Поды приложений будут предпочитать размещаться на одних и тех же нодах
- **Гибкость**: `preferredDuringSchedulingIgnoredDuringExecution` не требует жесткого размещения
- **Оптимизация**: Лучшее использование ресурсов на меньшем количестве нод

### Применение

1. Добавить affinity ко всем основным deployment:
   - `arch-repo-service`
   - `tenant-service`
   - `portal`
   - `hasura`
   - `tolgee`
   - `mailpit`
   - `kratos`
   - `hydra`
   - `oathkeeper`

2. После применения изменений через ArgoCD, поды будут перераспределены

3. Проверить размещение:
   ```bash
   kubectl get pods -o wide -A | grep -E "arch-repo|tenant|portal|hasura|tolgee|mailpit|kratos|hydra|oathkeeper"
   ```

### Альтернативный подход: nodeSelector

Если нужно жестко закрепить поды на конкретных нодах:

```yaml
spec:
  template:
    spec:
      nodeSelector:
        workload: application  # Ноды с этим label
```

Но это менее гибко и требует предварительной настройки labels на нодах.

### Мониторинг

После применения изменений проверьте:

1. **Распределение подов по нодам:**
   ```bash
   kubectl get pods -o wide -A | awk '{print $7}' | sort | uniq -c
   ```

2. **Использование ресурсов нодами:**
   ```bash
   kubectl top nodes
   ```

3. **Запрошенные ресурсы:**
   ```bash
   kubectl describe nodes | grep -A 10 "Allocated resources"
   ```

## Ожидаемый результат

После применения изменений:
- Основные поды приложений будут сгруппированы на 3-4 нодах
- Остальные 2-3 ноды будут использоваться для системных подов и резерва
- Лучшее использование ресурсов и более эффективное управление

## Удаление высвободившихся нод

### Важно: Ноды НЕ удаляются автоматически

Kubernetes не удаляет ноды автоматически. Высвободившиеся ноды останутся в кластере, но на них не будут размещаться поды приложений (благодаря `podAffinity`).

### Как удалить ноды вручную

1. **Проверьте, что на нодах нет подов приложений:**
   ```bash
   # Проверьте распределение подов по нодам
   kubectl get pods -o wide -A | grep -E "node-5|node-6" | grep -v "kube-system"
   ```

2. **Убедитесь, что на нодах только системные поды:**
   - Системные поды (kube-system, traefik, argocd) могут оставаться на всех нодах
   - Это нормально, они не занимают много ресурсов

3. **Удалите ноды через панель управления TimeWeb:**
   - Зайдите в панель управления TimeWeb
   - Найдите ваш Kubernetes кластер
   - Выберите ноды 5 и 6
   - Удалите их через интерфейс

### Рекомендации перед удалением

1. **Подождите 1-2 дня** после применения изменений, чтобы убедиться, что все работает стабильно
2. **Проверьте метрики** использования ресурсов на оставшихся нодах
3. **Убедитесь, что есть резерв** ресурсов на случай пиковых нагрузок

### Что будет после удаления

- Поды приложений останутся на 3-4 нодах
- Системные поды перераспределятся на оставшиеся ноды
- Кластер станет более эффективным и экономичным

### Альтернатива: Оставить ноды как резерв

Если хотите оставить ноды как резерв для масштабирования:
- Ноды останутся в кластере, но не будут использоваться для приложений
- При необходимости можно будет быстро разместить на них поды
- Это увеличит стоимость, но даст гибкость
